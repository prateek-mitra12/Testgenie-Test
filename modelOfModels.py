import os
import json
from neo4j import GraphDatabase
from dotenv import load_dotenv
import boto3
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize


from dotenv import load_dotenv
load_dotenv()


aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
aws_secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
aws_default_region = os.getenv("AWS_DEFAULT_REGION")

bedrock_runtime = boto3.client(
    'bedrock-runtime',
    region_name=aws_default_region,
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key
)


# # Wait 60 seconds before connecting using these details, or login to https://console.neo4j.io to validate the Aura Instance is available
# NEO4J_URI=neo4j+s://edebba48.databases.neo4j.io
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=gjC2zNxRLqgZE3hdB4-UhNszVeorXezvL438kBWbOAk
# AURA_INSTANCEID=edebba48
# AURA_INSTANCENAME=Instance01

NEO4J_URI = "neo4j+ssc://edebba48.databases.neo4j.io"
NEO4J_AUTH = ("neo4j", "gjC2zNxRLqgZE3hdB4-UhNszVeorXezvL438kBWbOAk")
NEO4J_USERNAME = "neo4j"
NEO4J_PASSWORD = "gjC2zNxRLqgZE3hdB4-UhNszVeorXezvL438kBWbOAk"


# # Create the driver instance
# driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)

# try:
#     driver.verify_connectivity()  # Check connection
#     print("Connected successfully!")
# except Exception as e:
#     print(f"Connection failed: {e}")
# finally:
#     driver.close()


class Neo4jDB:
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))


    def close(self):
        self.driver.close()


    def store_response(self, user_query, response_text, model_name):
        """Stores response as nodes in Neo4j with relationships."""
        query_embedding = get_bedrock_embeddings(user_query)
        with self.driver.session() as session:
            session.run(
                """
                MERGE (q:Query {text: $user_query, embedding: $embedding})
                MERGE (r:Fact {text: $response_text, model: $model_name})
                MERGE (r)-[:BELONGS_TO]->(q)
                """,
                user_query=user_query,
                response_text=response_text,
                model_name=model_name,
                embedding=query_embedding
            )
        print("Response Stored Successfully !!")


    def retrieve_relevant_facts(self, user_query):
        query_embedding = get_bedrock_embeddings(user_query)
        
        # Retrieve all stored queries and their embeddings
        with self.driver.session() as session:
            results = session.run(
                """
                MATCH (q:Query)-[:BELONGS_TO]-(r:Fact)
                RETURN q.text AS query_text, q.embedding AS query_embedding, r.text AS fact_text
                """
            )

            records = list(results)
        
        relevant_facts = []
        for record in records:
            stored_query_embedding = record["query_embedding"]
            similarity = compute_similarity(user_query, record["query_text"], query_embedding, stored_query_embedding)
            
            if similarity > 0.8:
                relevant_facts.append(record["fact_text"])

        return relevant_facts


    def get_final_response(self, user_query, relevant_facts):
        merge_prompt = (
            f"For the query : {user_query}, there is a list of responses generated by different LLM models. "
            f"List of Responses : {relevant_facts}"
            "Merge these results such that the final result contains all the information of both the responses. "
            "Don't mention things like this : Certainly! I'll merge the test cases from both responses. "
            "Also, make sure, for the duplicate information, keep it only single time in the final response. "
            
            "Important : In final response, provide SQL queries in code blocks using triple backticks (```)."
            "   Example Test Case Format:\n"

            "   Test Case 1: [Description]\n"
            "   SQL Query:\n"
            "   ```sql\n"
            "   SELECT * FROM table WHERE condition;\n"
            "   ```\n\n"
            
            "   Test Case 2: Verify retrieval of all active users.\n"
            "   **SQL Query:**\n"
            "   ```sql\n"
            "   SELECT * FROM users WHERE status = 'active';\n"
            "   ```\n\n"

            "2. For non-SQL requests (e.g., generating test data in tabular format):\n"
            "   - Provide structured test data in markdown table format.\n"
            "   - Use appropriate formatting (e.g., markdown tables for tabular data).\n"

            "   Example Format:\n"
            "   | Column1 | Column2 | Column3 |\n"
            "   |---------|---------|--------|\n"
            "   | Data1   | Data2   | Data3   |\n\n"
        )
        return llm_response(merge_prompt, "anthropic.claude-3-5-sonnet-20240620-v1:0")
    


def get_bedrock_embeddings(text):
    if not isinstance(text, str):
        print(f"Invalid input type for embeddings: Expected string but got {type(text)}")
        return None
    
    try:
        response = bedrock_runtime.invoke_model(
            # modelId="amazon.titan-embed-text-v2:0",
            modelId="amazon.titan-embed-text-v1",
            contentType="application/json",
            accept="application/json",
            body=json.dumps({"inputText": text})
        )

        response_body = json.loads(response['body'].read())

        if 'embedding' in response_body:
            return response_body['embedding']
        else:
            raise ValueError(f"Unexpected response from Bedrock: {response_body}")
    except Exception as e:
        print(f"Error fetching embedding for text: {e}")
        return None
    

def jaccard_similarity(sentence1, sentence2):
    words1 = set(sentence1.split())
    words2 = set(sentence2.split())
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    return float(len(intersection)) / len(union) if union else 0.0


def compute_similarity(sentence1, sentence2, embedding1, embedding2):
    # if sentence1:
    #     sentence1 = preprocess_text(sentence1)
    # if sentence2:
    #     sentence2 = preprocess_text(sentence2)

    if sentence1 == "" and sentence2 == "":
        return 1.0

    if sentence1 and sentence2:
        embedding1 = np.array(embedding1).reshape(1, -1)
        embedding2 = np.array(embedding2).reshape(1, -1)

        # Normalize embeddings before computing cosine similarity
        embedding1 = normalize(embedding1, axis=1)
        embedding2 = normalize(embedding2, axis=1)

        embedding_similarity = cosine_similarity(embedding1, embedding2)[0][0]
        lexical_similarity = jaccard_similarity(sentence1, sentence2)

        combined_similarity = 0.9*embedding_similarity + 0.1*lexical_similarity

        return combined_similarity

    return 0.0


# Initialize Neo4j
neo4j_db = Neo4jDB(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)


def modelOfModels(user_query):
    """
    Generates responses using multiple LLM models and merges relevant sections into a single response.
    """
    response1 = llm_response(user_query, "anthropic.claude-3-5-sonnet-20240620-v1:0")
    response2 = llm_response(user_query, "anthropic.claude-3-sonnet-20240229-v1:0")

    if not response1 and not response2:
        return "No response generated."
    
    print(f"Response 1 = {response1} \n\n")
    print(f"Response 2 = {response2} \n\n")

    # # Store responses in Neo4j
    # if response1:
    #     neo4j_db.store_response(user_query, response1, "Claude-3.5")
    # if response2:
    #     neo4j_db.store_response(user_query, response2, "Claude-3")

    # relevant_facts = neo4j_db.retrieve_relevant_facts(user_query)

    # final_response = neo4j_db.get_final_response(user_query, relevant_facts)

    # print("\n\n Final Response --> \n")
    # print(final_response)

    # # Store final response
    # neo4j_db.store_response(user_query, final_response, "Final-Response")

    # return final_response
    return merge_model_responses(response1, response2)


def llm_response(user_query, model_ID):
    prompt = (
        f"User Query : {user_query} "
    )

    response = bedrock_runtime.invoke_model(
        modelId=f"{model_ID}",
        contentType="application/json",
        accept="*/*",
        body= json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 200000,
            "top_k": 250,
            "stop_sequences": [],
            "temperature": 1,
            "top_p": 0.999,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": f"{prompt}"
                        }
                    ]
                }
            ]
        })
    )

    if response.get("body"):
        response_body = response.get("body")
        response_data = json.loads(response_body.read())
        completion = response_data.get("content")[0].get("text")

        if completion:
            return completion
        else:
            return None
    else:
        return None




# Example usage
if __name__ == "__main__":
    user_query = "Describe Food chain ?"
    final_answer = modelOfModels(user_query)

    # Close Neo4j connection
    neo4j_db.close()


import re

def extract_sections(response):
    """
    Extracts introduction, SQL test cases, and conclusion separately from the response.
    """
    lines = response.strip().split("\n")

    # Find the first and last SQL query
    sql_start, sql_end = None, None
    for i, line in enumerate(lines):
        if re.match(r"```sql", line.strip()):
            if sql_start is None:
                sql_start = i
            sql_end = i  # Keep updating to find the last occurrence

    # Separate intro, SQL block, and conclusion
    intro = "\n".join(lines[:sql_start]).strip() if sql_start else response
    sql_queries = "\n".join(lines[sql_start:sql_end+2]).strip() if sql_start is not None else ""
    conclusion = "\n".join(lines[sql_end+2:]).strip() if sql_end and sql_end+2 < len(lines) else ""

    return intro, sql_queries, conclusion


def extract_sql_queries(sql_block):
    """
    Extracts individual SQL queries from a block of SQL code.
    """
    sql_queries = re.findall(r"```sql\n(.*?)\n```", sql_block, re.DOTALL)
    return set(sql_queries)  # Deduplicate queries


def merge_model_responses(response1, response2):
    """
    Merges responses from two models by combining intro, deduplicating SQL queries, and merging conclusions.
    """
    intro1, sql_block1, conclusion1 = extract_sections(response1)
    intro2, sql_block2, conclusion2 = extract_sections(response2)

    print("For response 1")
    print(intro1)
    print(sql_block1)
    print(conclusion1)

    # Deduplicate SQL queries
    sql_queries_1 = extract_sql_queries(sql_block1)
    sql_queries_2 = extract_sql_queries(sql_block2)
    merged_sql_queries = sorted(sql_queries_1.union(sql_queries_2))  # Union to remove duplicates

    # Reconstruct the final SQL block
    merged_sql_block = "\n\n".join([f"```sql\n{query}\n```" for query in merged_sql_queries])

    # Choose the longest intro & conclusion for more informative content
    final_intro = intro1 if len(intro1) > len(intro2) else intro2
    final_conclusion = conclusion1 if len(conclusion1) > len(conclusion2) else conclusion2

    # Construct the final response
    final_response = f"{final_intro}\n\n{merged_sql_block}\n\n{final_conclusion}".strip()
    return final_response

























##########################################################################################################################
"""

how graph database is better than using dict or list

ChatGPT said:
Graph databases, such as Neo4j, are better than using dictionaries or lists in scenarios involving highly interconnected data. Here‚Äôs why:

1. Relationships Are First-Class Citizens
‚úÖ Graph DB: Relationships are explicitly stored and indexed.
Example: If you need to find related test cases, Neo4j can quickly traverse SIMILAR_TO relationships.

Neo4j Query:

cypher
Copy
Edit
MATCH (t1:TestCase)-[:SIMILAR_TO]->(t2:TestCase) RETURN t1, t2
‚ùå Dict/List: You need nested loops to check relationships manually.
python
Copy
Edit
for case1 in test_cases:
    for case2 in test_cases:
        if are_similar(case1, case2):  # Custom logic
            similar_cases.append((case1, case2))
This does not scale well as data grows.
2. Query Performance (Speed)
‚úÖ Graph DB: Uses index-free adjacency, meaning nodes store direct pointers to related nodes ‚Üí Fast lookups
Example: Find all test cases generated by a model in constant time O(1).
cypher
Copy
Edit
MATCH (m:Model {name: 'Claude-3'})-[:GENERATED]->(t:TestCase) RETURN t
‚ùå Dict/List:
Using a list requires O(n) linear search every time.
Using a dict requires manual indexing:
python
Copy
Edit
test_cases_by_model = {
    "Claude-3": [test1, test2, test3]
}
This approach works but requires more memory and manual management.
3. Flexibility in Data Structure
‚úÖ Graph DB: Schema-free, easily handles new relationships
Example: If tomorrow you need to group test cases by execution environment, you can add a relationship dynamically.
c
Copy
Edit
MATCH (t:TestCase)
MERGE (env:Environment {name: 'Production'})
MERGE (t)-[:EXECUTED_IN]->(env)
‚ùå Dict/List: Requires modifying the entire structure.
python
Copy
Edit
for test_case in test_cases:
    test_case['environment'] = 'Production'
If test_case was originally just a string, you now need to refactor.
4. Merging Data (Like Your Use Case)
‚úÖ Graph DB: Can efficiently merge results from two models
If two models generate similar SQL queries, Neo4j automatically avoids duplication:
cypher
Copy
Edit
MATCH (t1:TestCase)-[:SIMILAR_TO]->(t2:TestCase)
RETURN DISTINCT t1.text
Graph traversal is fast because only similar nodes are checked.
‚ùå Dict/List: Requires manual merging logic:
python
Copy
Edit
unique_cases = set()
for case1 in model1_cases:
    for case2 in model2_cases:
        if compute_similarity(case1, case2) > 0.85:
            unique_cases.add(case1)
This is expensive (O(n^2)) and doesn‚Äôt scale.
5. Real-World Use Cases for Graph DB
‚úÖ When to Use a Graph DB

Storing test case relationships (SIMILAR_TO, BELONGS_TO_MODEL)
Complex queries: "Find all test cases related to a failed test in production."
Social networks: Connections between people, recommendations
Fraud detection: Finding fraudulent transactions in banking
Supply chain optimization: Tracking product movement efficiently
‚ùå When Dict/List Is Enough

Flat, small datasets: A list or dict is enough if there‚Äôs no need for relationships.
Simple lookups: If you always search by one key (like id ‚Üí test_case), a dict works fine.
Conclusion
Graph DBs (like Neo4j) are better than lists/dicts when:
‚úÖ You need fast lookups on relationships
‚úÖ You need efficient merging of related test cases
‚úÖ You want flexibility to add new relationships

If your test cases are highly interconnected, Neo4j is the better choice üöÄ.

"""
##########################################################################################################################






