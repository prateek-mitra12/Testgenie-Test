import os
import json
from neo4j import GraphDatabase
from dotenv import load_dotenv
import boto3
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize


from dotenv import load_dotenv
load_dotenv()


aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
aws_secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
aws_default_region = os.getenv("AWS_DEFAULT_REGION")

bedrock_runtime = boto3.client(
    'bedrock-runtime',
    region_name=aws_default_region,
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key
)


# # Wait 60 seconds before connecting using these details, or login to https://console.neo4j.io to validate the Aura Instance is available
# NEO4J_URI=neo4j+s://edebba48.databases.neo4j.io
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=gjC2zNxRLqgZE3hdB4-UhNszVeorXezvL438kBWbOAk
# AURA_INSTANCEID=edebba48
# AURA_INSTANCENAME=Instance01

NEO4J_URI = "neo4j+ssc://edebba48.databases.neo4j.io"
NEO4J_AUTH = ("neo4j", "gjC2zNxRLqgZE3hdB4-UhNszVeorXezvL438kBWbOAk")
NEO4J_USERNAME = "neo4j"
NEO4J_PASSWORD = "gjC2zNxRLqgZE3hdB4-UhNszVeorXezvL438kBWbOAk"


# # Create the driver instance
# driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)

# try:
#     driver.verify_connectivity()  # Check connection
#     print("Connected successfully!")
# except Exception as e:
#     print(f"Connection failed: {e}")
# finally:
#     driver.close()


class Neo4jDB:
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))


    def close(self):
        self.driver.close()


    def store_response(self, user_query, response_text, model_name):
        """Stores response as nodes in Neo4j with relationships."""
        query_embedding = get_bedrock_embeddings(user_query)
        with self.driver.session() as session:
            session.run(
                """
                MERGE (q:Query {text: $user_query, embedding: $embedding})
                MERGE (r:Fact {text: $response_text, model: $model_name})
                MERGE (r)-[:BELONGS_TO]->(q)
                """,
                user_query=user_query,
                response_text=response_text,
                model_name=model_name,
                embedding=query_embedding
            )
        print("Response Stored Successfully !!")


    def retrieve_relevant_facts(self, user_query):
        query_embedding = get_bedrock_embeddings(user_query)
        
        # Retrieve all stored queries and their embeddings
        with self.driver.session() as session:
            results = session.run(
                """
                MATCH (q:Query)-[:BELONGS_TO]-(r:Fact)
                RETURN q.text AS query_text, q.embedding AS query_embedding, r.text AS fact_text
                """
            )

            records = list(results)
        
        relevant_facts = []
        for record in records:
            stored_query_embedding = record["query_embedding"]
            similarity = compute_similarity(user_query, record["query_text"], query_embedding, stored_query_embedding)
            
            if similarity > 0.8:
                relevant_facts.append(record["fact_text"])

        return relevant_facts


    def get_final_response(self, user_query, relevant_facts):
        merge_prompt = (
            f"For the query : {user_query}, there is a list of responses generated by different LLM models. "
            f"List of Responses : {relevant_facts}"
            "Merge these results such that the final result contains all the information of both the responses. "
            "Don't mention things like this : Certainly! I'll merge the test cases from both responses. "
            "Also, make sure, for the duplicate information, keep it only single time in the final response. "
            
            "Important : In final response, provide SQL queries in code blocks using triple backticks (```)."
            "   Example Test Case Format:\n"

            "   Test Case 1: [Description]\n"
            "   SQL Query:\n"
            "   ```sql\n"
            "   SELECT * FROM table WHERE condition;\n"
            "   ```\n\n"
            
            "   Test Case 2: Verify retrieval of all active users.\n"
            "   **SQL Query:**\n"
            "   ```sql\n"
            "   SELECT * FROM users WHERE status = 'active';\n"
            "   ```\n\n"

            "2. For non-SQL requests (e.g., generating test data in tabular format):\n"
            "   - Provide structured test data in markdown table format.\n"
            "   - Use appropriate formatting (e.g., markdown tables for tabular data).\n"

            "   Example Format:\n"
            "   | Column1 | Column2 | Column3 |\n"
            "   |---------|---------|--------|\n"
            "   | Data1   | Data2   | Data3   |\n\n"
        )
        return llm_response(merge_prompt, "anthropic.claude-3-5-sonnet-20240620-v1:0")
    


def get_bedrock_embeddings(text):
    if not isinstance(text, str):
        print(f"Invalid input type for embeddings: Expected string but got {type(text)}")
        return None
    
    try:
        response = bedrock_runtime.invoke_model(
            # modelId="amazon.titan-embed-text-v2:0",
            modelId="amazon.titan-embed-text-v1",
            contentType="application/json",
            accept="application/json",
            body=json.dumps({"inputText": text})
        )

        response_body = json.loads(response['body'].read())

        if 'embedding' in response_body:
            return response_body['embedding']
        else:
            raise ValueError(f"Unexpected response from Bedrock: {response_body}")
    except Exception as e:
        print(f"Error fetching embedding for text: {e}")
        return None
    

def jaccard_similarity(sentence1, sentence2):
    words1 = set(sentence1.split())
    words2 = set(sentence2.split())
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    return float(len(intersection)) / len(union) if union else 0.0


def compute_similarity(sentence1, sentence2, embedding1, embedding2):
    # if sentence1:
    #     sentence1 = preprocess_text(sentence1)
    # if sentence2:
    #     sentence2 = preprocess_text(sentence2)

    if sentence1 == "" and sentence2 == "":
        return 1.0

    if sentence1 and sentence2:
        embedding1 = np.array(embedding1).reshape(1, -1)
        embedding2 = np.array(embedding2).reshape(1, -1)

        # Normalize embeddings before computing cosine similarity
        embedding1 = normalize(embedding1, axis=1)
        embedding2 = normalize(embedding2, axis=1)

        embedding_similarity = cosine_similarity(embedding1, embedding2)[0][0]
        lexical_similarity = jaccard_similarity(sentence1, sentence2)

        combined_similarity = 0.9*embedding_similarity + 0.1*lexical_similarity

        return combined_similarity

    return 0.0


# Initialize Neo4j
neo4j_db = Neo4jDB(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)


def modelOfModels(user_query):
    """
    Generates responses using multiple LLM models and merges relevant sections into a single response.
    """
    response1 = llm_response(user_query, "anthropic.claude-3-5-sonnet-20240620-v1:0")
    response2 = llm_response(user_query, "anthropic.claude-3-sonnet-20240229-v1:0")

    if not response1 and not response2:
        return "No response generated."
    
    print(f"Response 1 = {response1} \n\n")
    print(f"Response 2 = {response2} \n\n")

    # # Store responses in Neo4j
    # if response1:
    #     neo4j_db.store_response(user_query, response1, "Claude-3.5")
    # if response2:
    #     neo4j_db.store_response(user_query, response2, "Claude-3")

    # relevant_facts = neo4j_db.retrieve_relevant_facts(user_query)

    # final_response = neo4j_db.get_final_response(user_query, relevant_facts)

    # print("\n\n Final Response --> \n")
    # print(final_response)

    # # Store final response
    # neo4j_db.store_response(user_query, final_response, "Final-Response")

    # return final_response
    return merge_model_responses(response1, response2)


def llm_response(user_query, model_ID):
    prompt = (
        f"User Query : {user_query} "
    )

    response = bedrock_runtime.invoke_model(
        modelId=f"{model_ID}",
        contentType="application/json",
        accept="*/*",
        body= json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 200000,
            "top_k": 250,
            "stop_sequences": [],
            "temperature": 1,
            "top_p": 0.999,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": f"{prompt}"
                        }
                    ]
                }
            ]
        })
    )

    if response.get("body"):
        response_body = response.get("body")
        response_data = json.loads(response_body.read())
        completion = response_data.get("content")[0].get("text")

        if completion:
            return completion
        else:
            return None
    else:
        return None




# Example usage
if __name__ == "__main__":
    user_query = "Describe Food chain ?"
    final_answer = modelOfModels(user_query)

    # Close Neo4j connection
    neo4j_db.close()


import re

def extract_sections(response):
    """
    Extracts introduction, SQL test cases, and conclusion separately from the response.
    """
    lines = response.strip().split("\n")

    # Find the first and last SQL query
    sql_start, sql_end = None, None
    for i, line in enumerate(lines):
        if re.match(r"```sql", line.strip()):
            if sql_start is None:
                sql_start = i
            sql_end = i  # Keep updating to find the last occurrence

    # Separate intro, SQL block, and conclusion
    intro = "\n".join(lines[:sql_start]).strip() if sql_start else response
    sql_queries = "\n".join(lines[sql_start:sql_end+2]).strip() if sql_start is not None else ""
    conclusion = "\n".join(lines[sql_end+2:]).strip() if sql_end and sql_end+2 < len(lines) else ""

    return intro, sql_queries, conclusion


def extract_sql_queries(sql_block):
    """
    Extracts individual SQL queries from a block of SQL code.
    """
    sql_queries = re.findall(r"```sql\n(.*?)\n```", sql_block, re.DOTALL)
    return set(sql_queries)  # Deduplicate queries


def merge_model_responses(response1, response2):
    """
    Merges responses from two models by combining intro, deduplicating SQL queries, and merging conclusions.
    """
    intro1, sql_block1, conclusion1 = extract_sections(response1)
    intro2, sql_block2, conclusion2 = extract_sections(response2)

    print("For response 1")
    print(intro1)
    print(sql_block1)
    print(conclusion1)

    # Deduplicate SQL queries
    sql_queries_1 = extract_sql_queries(sql_block1)
    sql_queries_2 = extract_sql_queries(sql_block2)
    merged_sql_queries = sorted(sql_queries_1.union(sql_queries_2))  # Union to remove duplicates

    # Reconstruct the final SQL block
    merged_sql_block = "\n\n".join([f"```sql\n{query}\n```" for query in merged_sql_queries])

    # Choose the longest intro & conclusion for more informative content
    final_intro = intro1 if len(intro1) > len(intro2) else intro2
    final_conclusion = conclusion1 if len(conclusion1) > len(conclusion2) else conclusion2

    # Construct the final response
    final_response = f"{final_intro}\n\n{merged_sql_block}\n\n{final_conclusion}".strip()
    return final_response

























##########################################################################################################################
"""

how graph database is better than using dict or list

ChatGPT said:
Graph databases, such as Neo4j, are better than using dictionaries or lists in scenarios involving highly interconnected data. Here’s why:

1. Relationships Are First-Class Citizens
✅ Graph DB: Relationships are explicitly stored and indexed.
Example: If you need to find related test cases, Neo4j can quickly traverse SIMILAR_TO relationships.

Neo4j Query:

cypher
Copy
Edit
MATCH (t1:TestCase)-[:SIMILAR_TO]->(t2:TestCase) RETURN t1, t2
❌ Dict/List: You need nested loops to check relationships manually.
python
Copy
Edit
for case1 in test_cases:
    for case2 in test_cases:
        if are_similar(case1, case2):  # Custom logic
            similar_cases.append((case1, case2))
This does not scale well as data grows.
2. Query Performance (Speed)
✅ Graph DB: Uses index-free adjacency, meaning nodes store direct pointers to related nodes → Fast lookups
Example: Find all test cases generated by a model in constant time O(1).
cypher
Copy
Edit
MATCH (m:Model {name: 'Claude-3'})-[:GENERATED]->(t:TestCase) RETURN t
❌ Dict/List:
Using a list requires O(n) linear search every time.
Using a dict requires manual indexing:
python
Copy
Edit
test_cases_by_model = {
    "Claude-3": [test1, test2, test3]
}
This approach works but requires more memory and manual management.
3. Flexibility in Data Structure
✅ Graph DB: Schema-free, easily handles new relationships
Example: If tomorrow you need to group test cases by execution environment, you can add a relationship dynamically.
c
Copy
Edit
MATCH (t:TestCase)
MERGE (env:Environment {name: 'Production'})
MERGE (t)-[:EXECUTED_IN]->(env)
❌ Dict/List: Requires modifying the entire structure.
python
Copy
Edit
for test_case in test_cases:
    test_case['environment'] = 'Production'
If test_case was originally just a string, you now need to refactor.
4. Merging Data (Like Your Use Case)
✅ Graph DB: Can efficiently merge results from two models
If two models generate similar SQL queries, Neo4j automatically avoids duplication:
cypher
Copy
Edit
MATCH (t1:TestCase)-[:SIMILAR_TO]->(t2:TestCase)
RETURN DISTINCT t1.text
Graph traversal is fast because only similar nodes are checked.
❌ Dict/List: Requires manual merging logic:
python
Copy
Edit
unique_cases = set()
for case1 in model1_cases:
    for case2 in model2_cases:
        if compute_similarity(case1, case2) > 0.85:
            unique_cases.add(case1)
This is expensive (O(n^2)) and doesn’t scale.
5. Real-World Use Cases for Graph DB
✅ When to Use a Graph DB

Storing test case relationships (SIMILAR_TO, BELONGS_TO_MODEL)
Complex queries: "Find all test cases related to a failed test in production."
Social networks: Connections between people, recommendations
Fraud detection: Finding fraudulent transactions in banking
Supply chain optimization: Tracking product movement efficiently
❌ When Dict/List Is Enough

Flat, small datasets: A list or dict is enough if there’s no need for relationships.
Simple lookups: If you always search by one key (like id → test_case), a dict works fine.
Conclusion
Graph DBs (like Neo4j) are better than lists/dicts when:
✅ You need fast lookups on relationships
✅ You need efficient merging of related test cases
✅ You want flexibility to add new relationships

If your test cases are highly interconnected, Neo4j is the better choice 🚀.

"""
##########################################################################################################################






